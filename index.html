<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap</title>
  <meta name="description" content="Project page for GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap">
  <meta property="og:type" content="website">
  <meta property="og:title" content="GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap"/>
  <meta property="og:description" content="GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap"/>
  <meta property="og:url" content="https://donghwijung.github.io/GOTPR/"/>
  <!-- <meta property="og:image" content="static/images/concept.jpg" /> -->
  <!-- <meta property="og:image:width" content="3852"/>
  <meta property="og:image:height" content="981"/> -->

  <meta name="twitter:title" content="GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap">
  <meta name="twitter:description" content="Project page for GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap">
  <!-- <meta name="twitter:image" content="static/images/concept.jpg"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <meta name="keywords" content="text-based place recognition, scene graph, OpenStreetMap">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap</title>
  <link rel="icon" type="image/x-icon" href="static/images/magnifying_map.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GOTPR: General Outdoor Text-based Place Recognition Using Scene Graph Retrieval with OpenStreetMap</h1>
            <!-- <h2 class="title is-5 publlication-title">Submitted to </h2> -->
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://donghwijung.github.io/" target="https://donghwijung.github.io/">Donghwi Jung</a>,</span>
              <span class="author-block"><a href="https://www.notion.so/Keonwoo-Kim-743fdb8532e34542bca4172790183849?pvs=4" target="https://www.notion.so/Keonwoo-Kim-743fdb8532e34542bca4172790183849?pvs=4">Keonwoo Kim</a>,</span>
              <span class="author-block"><a href="https://arisnu.squarespace.com/director" target="https://arisnu.squarespace.com/director">Seong-Woo Kim</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <small>
                  <br> Seoul National University
                </small>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv abstract link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.08575" target="https://arxiv.org/abs/2501.08575"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Preprint</span>
                  </a>
                </span>
                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/donghwijung/GOTPR" target="https://github.com/donghwijung/GOTPR"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Youtube link -->
                <!-- <span class="link-block">
                  <a href="https://youtu.be/59K14A1NqNw?si=-aKJ2l4cAAXT3K6L" target="https://youtu.be/59K14A1NqNw?si=-aKJ2l4cAAXT3K6L"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-video"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose GOTPR, a robust place recognition method designed for outdoor environments where GPS signals are unavailable. Unlike existing approaches that use point cloud maps, which are large and difficult to store, GOTPR leverages scene graphs generated from text descriptions and maps for place recognition. This method improves scalability by replacing point clouds with compact data structures, allowing robots to efficiently store and utilize extensive map data. Additionally, GOTPR eliminates the need for custom map creation by using publicly available OpenStreetMap data, which provides global spatial information. We evaluated its performance using the KITTI360Pose dataset with corresponding OpenStreetMap data, comparing it to existing point cloud-based place recognition methods. The results show that GOTPR achieves comparable ac- curacy while significantly reducing storage requirements. In city- scale tests, it completed processing within a few seconds, making it highly practical for real-world robotics applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Proposed Method Video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/gotpr_video.mp4" type="video/mp4">
      </video>
      <!-- <div class="level-set has-text-justified">
        <p>
          Voice narration is also provided. If you would like to watch the video with audio, please make sure to unmute it before viewing.
        </p>
      </div> -->
    </div>
  </div>
</section>
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="tree" autoplay loop muted playsinline width="100%">
        <source src="./static/videos/gotpr_video.mp4" type="video/mp4">
      </video>
      <div class="level-set has-text-justified">
        <p>
          Voice narration is also provided. If you would like to watch the video with audio, please make sure to unmute it before viewing.
        </p>
      </div>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Concept</h2>
          <center>
            <img src="static/images/concept.jpg" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Concept of GOTPR. The user describes the surrounding environment in text and provides it to the delivery robot. The robot interprets the text description to determine the user's scene. In this process, both the text description and the OSM data are converted into scene graphs. The robot then retrieves and identifies the most similar pair, estimating the current scene based on the highest similarity.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Related works</h2>
          <center>
            <img src="static/images/related_works.png" class="center-image blend-img-background" style="width:75%;"/>
          </center>
          <!-- <div class="level-set has-text-justified"> -->
            <!-- <p>
              Comparison with the previous text-based place recognition works
            </p> -->
          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Process</h2>
          <center>
            <img src="static/images/process.jpg" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Process of GOTPR. The method consists of three sequential steps: 1) Scene graph generation, 2) Scene graph candidates extraction, 3) Scene graph retrieval and scene selection. The input data are a query text description and OSM. The output is the matching scene id.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <h2 class="title is-3">Scene graph generation</h2>
        <div class="content">
          <h2 class="title is-4">OSM scene graph generation</h2>
          <center>
            <img src="static/images/osm_map_scene_graph_generation_process.jpg" class="center-image blend-img-background"/>
          </center>
          <!-- <div class="level-set has-text-justified"> -->
            <!-- <p>
            </p> -->
          <!-- </div> -->
        </div>

        <div class="content">
          <h2 class="title is-4">Text scene graph generation</h2>
          <center>
            <img src="static/images/text_scene_graph_generation_process.jpg" class="center-image blend-img-background"/>
          </center>
          <!-- <div class="level-set has-text-justified"> -->
            <!-- <p>
            </p> -->
          <!-- </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Scene graph retrieval network</h2>
          <center>
            <img src="static/images/scene_retrieval_network.jpg" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              Network of scene graph retrieval. Input of the process are query text scene graph and the extracted OSM scene graph candidates. And the output is the selected top-k scene id. The joint embedding model consists of multiple GPS convolution layers with self and cross modules.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Experiments</h2>
          <div class="content">
            <h2 class="title is-4">Experimental example</h2>
            <center>
              <img src="static/images/experimental_example.jpg" class="center-image blend-img-background"/>
            </center>
            <div class="level-set has-text-justified">
              <p>
                Examples of the experimental data are provided. The data were generated based on GPS coordinates 48.964117, 8.472481. (a) and (b) are presented to facilitate comparison and understanding rather than actual data used in GOTPR. In addition, The segmentations in (b) and (c) are included solely for visualization purposes to aid understanding. Moreover, the gray area in (f) represents the overlapping region with (e).
              </p>
            </div>
          </div>

          <div class="content">
            <h2 class="title is-4">City-scale data generation (GPS sampling)</h2>
            <center>
              <img src="static/images/gps_sampling_toronto.png" class="center-image blend-img-background" style="width:80%"/>
            </center>
            <div class="level-set has-text-centered">
              <p>
                GPS samples collected in Toronto (<span style="color:blue">⬤</span>: Included GPS, <span style="color:red">⬤</span>: Excluded GPS)
              </p>
            </div>
          </div>

          <div class="content">
            <h2 class="title is-4">A quantitative evaluation with baseline comparisons and ablation study</h2>
            <center>
              <img src="static/images/exp1_result.png" class="center-image blend-img-background"/>
            </center>
            <div class="level-set has-text-justified">
            </div>
          </div>

          <div class="content">
            <h2 class="title is-4">A quantitative evaluation for candidates extraction</h2>
            <center>
              <img src="static/images/exp2_result.png" class="center-image blend-img-background" style="width:50%;"/>
            </center>
            <!-- <div class="level-set has-text-justified"> -->
              <!-- <p>
                Examples of the experimental data are provided. The data were generated based on GPS coordinates 48.964117, 8.472481. (a) and (b) are presented to facilitate comparison and understanding rather than actual data used in GOTPR. In addition, The segmentations in (b) and (c) are included solely for visualization purposes to aid understanding. Moreover, the gray area in (f) represents the overlapping region with (e).
              </p> -->
            <!-- </div> -->
          </div>

          <div class="content">
            <h2 class="title is-4">A quantitative evaluation for city-scale data</h2>
            <center>
              <img src="static/images/exp3_result.png" class="center-image blend-img-background" style="width:50%"/>
            </center>
            <!-- <div class="level-set has-text-justified"> -->
              <!-- <p>
                Examples of the experimental data are provided. The data were generated based on GPS coordinates 48.964117, 8.472481. (a) and (b) are presented to facilitate comparison and understanding rather than actual data used in GOTPR. In addition, The segmentations in (b) and (c) are included solely for visualization purposes to aid understanding. Moreover, the gray area in (f) represents the overlapping region with (e).
              </p> -->
            <!-- </div> -->
          </div>

          <div class="content">
            <h2 class="title is-4">A quantitative evaluation of processing time for rule-based and transformer-based methods</h2>
            <center>
              <img src="static/images/processing_time.png" class="center-image blend-img-background" style="width:50%"/>
            </center>
            <!-- <div class="level-set has-text-justified"> -->
              <!-- <p>
                Examples of the experimental data are provided. The data were generated based on GPS coordinates 48.964117, 8.472481. (a) and (b) are presented to facilitate comparison and understanding rather than actual data used in GOTPR. In addition, The segmentations in (b) and (c) are included solely for visualization purposes to aid understanding. Moreover, the gray area in (f) represents the overlapping region with (e).
              </p> -->
            <!-- </div> -->
          </div>

          <div class="content">
            <h2 class="title is-4">A quantitative evaluation of the map data size in point clouds and scene graphs</h2>
            <center>
              <img src="static/images/map_data_size.png" class="center-image blend-img-background" style="width:50%"/>
            </center>
            <!-- <div class="level-set has-text-justified"> -->
              <!-- <p>
                Examples of the experimental data are provided. The data were generated based on GPS coordinates 48.964117, 8.472481. (a) and (b) are presented to facilitate comparison and understanding rather than actual data used in GOTPR. In addition, The segmentations in (b) and (c) are included solely for visualization purposes to aid understanding. Moreover, the gray area in (f) represents the overlapping region with (e).
              </p> -->
            <!-- </div> -->
          </div>

          <!-- <div class="content">
            <h2 class="title is-4">Qualitative results for GOTPR</h2>
            <center>
              <img src="static/images/map_data_size.png" class="center-image blend-img-background" style="width:50%"/>
            </center>
            <div class="level-set has-text-justified">
              <p>
                Examples of the experimental data are provided. The data were generated based on GPS coordinates 48.964117, 8.472481. (a) and (b) are presented to facilitate comparison and understanding rather than actual data used in GOTPR. In addition, The segmentations in (b) and (c) are included solely for visualization purposes to aid understanding. Moreover, the gray area in (f) represents the overlapping region with (e).
              </p>
            </div>
          </div> -->

        </div>
      </div>
      <!-- <h2 class="title is-3">
        Qualitative results
      </h2> -->
    </div>
    <h2 class="title is-4">Qualitative results</h2>
    <div class="columns is-centered">
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/text_description.jpg" alt="Image 1">
        </figure>
        <p class="subtitle is-6 mt-2">(a) Street-view image (360 degree).</p>
      </div>
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/query_text_graph.png" alt="Image 2">
        </figure>
        <p class="subtitle is-6 mt-2">(b) OSM image (used for directional reference).</p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/top1_osm(gt).png" alt="Image 3">
        </figure>
        <p class="subtitle is-6 mt-2">(c) Top-1 OSM (G.T.).</p>
      </div>
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/top1_osm_graph(gt).png" alt="Image 4">
        </figure>
        <p class="subtitle is-6 mt-2">(d) Top-1 OSM scene graph (G.T.).</p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/top3_osm.png" alt="Image 5">
        </figure>
        <p class="subtitle is-6 mt-2">(e) Top-3 OSM.</p>
      </div>
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/top3_osm_graph.png" alt="Image 6">
        </figure>
        <p class="subtitle is-6 mt-2">(f) Top-3 OSM scene graph.</p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/top5_osm.png" alt="Image 7">
        </figure>
        <p class="subtitle is-6 mt-2">(g) Top-5 OSM. </p>
      </div>
      <div class="column is-two-fifths has-text-centered">
        <figure class="image is-square">
          <img src="static/images/top5_osm_graph.png" alt="Image 8">
        </figure>
        <p class="subtitle is-6 mt-2">(h) Top-5 OSM scene graph.</p>
      </div>
    </div>
    <div class="level-set has-text-justified">
      <p>
        An example of place recognition results from GOTPR. The OSM scene graph that best matches the actual query text scene graph serves as the ground truth and corresponds to the Top-1 OSM scene graph with the highest similarity in the example above. The results were generated using the GPS coordinates 43.7594129828112,-79.46708294624517.
        The suffixes <i>_n1,2</i> attached to some node labels in (a) are identifiers used to distinguish nodes with the same label. These identifiers are removed during the text scene graph generation process, resulting in the text scene graph shown in (b).
      </p>
    </div>
  </div>
</section>

<!-- Prompt session -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="row">
      <h2 class="title is-3">
        Experiments involving the user-generated text description
      </h2>
      <div class="content">
        <h2 class="title is-4">Images presented to the tester</h2>
        <center>
          <img src="static/images/user_test_panoramic_streetview.jpg" class="center-image blend-img-background" style="width:100%"/>
        </center>
        <div class="level-set has-text-centered">
          <p>
            Street-view image (360 degree).
          </p>
        </div>
      </div>
      <div class="content">
        <center>
          <img src="static/images/user_test_osm.jpg" class="center-image blend-img-background" style="width:33%"/>
        </center>
        <div class="level-set has-text-centered">
          <p>
            OSM image (used for directional reference).
          </p>
        </div>
      </div>
      <p class="content has-text-justified">
          <a href="static/prompts/user_instruction.txt">User intruction</a> |
          <a href="static/prompts/user_text_description.txt">User-generated text description</a> |
          <a href="static/prompts/prompt.txt">Prompt for LLM (user-generated text to GOTPR format)</a> |
          <a href="static/prompts/llm_text_description.txt">LLM-generated text description</a>
        </p>
    </div>
  </div>
</section>

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{jung2025gotloc,
          title={GOTLoc: General Outdoor Text-based Localization Using Scene Graph Retrieval with OpenStreetMap},
          author={Jung, Donghwi and Kim, Keonwoo and Kim, Seong-Woo},
          journal={arXiv preprint arXiv:2501.08575},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- <section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
  This research was funded by the Korean Ministry of Land, Infrastructure and Transport (MOLIT) through the Smart City Innovative Talent Education Program and by the Korea Institute for Advancement of Technology (KIAT) under a MOTIE grant (P0020536). Additional support came from the Ministry of Education (MOE) and the National Research Foundation of Korea (NRF). K. Kim, D. Jung, and the corresponding author are affiliated with the Smart City Global Convergence program. Research facilities were provided by the Institute of Engineering Research at Seoul National University.  </div>
</section> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
